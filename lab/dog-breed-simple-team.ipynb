{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Azure Machine Learning services\n",
    "\n",
    "_Special thanks to the Azure CAT team and Daniel for the Dogbreeds sample._\n",
    "\n",
    "The purpose of this sample is to demonstrate:\n",
    "\n",
    "- What you need to start your run once the administrator has set up a workspaces for you\n",
    "- How to start your Azure Machine Learning services run \n",
    "\n",
    "This set up notebook shows only the first feature of \n",
    "See [Daniel's dogbreed notebook](../) for:\n",
    "\n",
    "- Distributed training\n",
    "- Hyperparameter tuning\n",
    "- Azure Machine Learning Pipelines\n",
    "- Inferencing\n",
    "- Deploy model as web service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you are using your own development system:\n",
    "\n",
    "1. Have Azure subscription\n",
    "2. Start Bash or PowerShell. [Can be Bash in Windows](https://docs.microsoft.com/en-us/windows/wsl/install-win10)\n",
    "3. Install (or update) [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest)\n",
    "4. Install [Conda](https://docs.anaconda.com/anaconda/install). Be sure Conda is on the PATH.\n",
    "5. Install [AzCopy](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10?toc=%2fazure%2fstorage%2fblobs%2ftoc.json)\n",
    "\n",
    "Or use a Linux Data Science Virtual Machine.\n",
    "\n",
    "\n",
    "### AzureML Python SDK\n",
    "\n",
    "Install and update the Python SDK. Install notebook and contrib\n",
    "\n",
    "```\n",
    "conda create -n azureml -y Python=3.6 ipywidgets nb_conda\n",
    "conda activate azureml\n",
    "pip install --upgrade azureml-sdk[notebooks,contrib] scikit-image tensorflow tensorboardX --user \n",
    "jupyter nbextension install --py --user azureml.widgets\n",
    "jupyter nbextension enable azureml.widgets --user --py\n",
    "```\n",
    "\n",
    "Install PyTorch:\n",
    "\n",
    "On MacOS: \n",
    "\n",
    "    conda install pytorch torchvision -c pytorch\n",
    "\n",
    "On Windows\n",
    "\n",
    "    conda install pytorch -c pytorch\n",
    "    pip install torchvision\n",
    "\n",
    "You will need to restart Jupyter.\n",
    "Detailed instructions are here: https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python \n",
    "\n",
    "### (Optional) Install VS Code and the VS Code extension \n",
    "\n",
    "[Download](https://code.visualstudio.com/) and install Visual Studio Code then the [Azure Machine Learning Extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.vscode-ai)\n",
    "Make sure it has a recent version of the Python SDK -- remove the folder ~/.azureml/envs if there are issuse. A current SDK will be installed when you first use AML from VSCode.\n",
    "\n",
    "### Clone this repository\n",
    "\n",
    "```\n",
    "git clone https://github.com/msraidli/dogbreeds\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System overview\n",
    "\n",
    "![overview](assets/overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admin gives you your workspace info:\n",
    "\n",
    "\n",
    "- SUBSCRIPTION_ID\n",
    "- RESOURCEGROUP_NAME\n",
    "- WORKSPACE_NAME\n",
    "- WORKSPACE_STORAGE_ACCOUNT\n",
    "- WORKSPACE_KEYVAULT\n",
    "\n",
    "Admin gives you the shared data location and key vault:\n",
    "\n",
    "- DATA_STORAGE_ACCOUNT\n",
    "- DATA_STORAGE_CONTAINER\n",
    "- DATA_KEY_VAULT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up script\n",
    "\n",
    "Run the following set up script for this notebook to securely access your data and Azure ML jobs.\n",
    "\n",
    "The set up script gets:\n",
    "\n",
    "- Storage account key for the shared data\n",
    "- Storage account key for your storage account\n",
    "\n",
    "```bash\n",
    "# dogbreeds data location\n",
    "export DATA_STORAGE_ACCOUNT=\"<shared data storage account>\"\n",
    "export DATA_STORAGE_CONTAINER=\"dogbreeds\" \n",
    "DATA_KEYVAULT_NAME=\"<shared data key vault>\"\n",
    "\n",
    "# your workspace information\n",
    "export SUBSCRIPTION_ID=\"<subscription id provided by admin\"\n",
    "export RESOURCEGROUP_NAME=\"<resource group name provided by admin>\"\n",
    "export WORKSPACE_NAME=\"<workspace name provided by admin>\"\n",
    "export WORKSPACE_STORAGE_ACCOUNT=\"<workspace's storage account provided by admin>\"\n",
    "WORKSPACE_KEYVAULT=\"<workspace's key vault name provided by admin>\"\n",
    "\n",
    "az login\n",
    "\n",
    "## log in using the browser\n",
    "\n",
    "az account set --subscription $SUBSCRIPTION_ID\n",
    "az account show\n",
    "\n",
    "# by convention the admin used the storage account as the name of the key stored key vault \n",
    "DATA_KEY=az keyvault secret show --name $DATA_STORAGE_ACCOUNT \\\n",
    "  --vault-name $DATA_KEYVAULT_NAME | \\\n",
    "python -c 'import sys, json; \\ sys.stdout.write(json.load(sys.stdin)[0][\\\" value\\\"])')\n",
    "\n",
    "export DATA_STORAGE_KEY=$DATA_KEY\n",
    "\n",
    "# by convention the admin used the storage account as the name of the key stored key vault \n",
    "STORAGE_KEY=az keyvault secret show --name $WORKSPACE_STORAGE_ACCOUNT \\\n",
    "  --vault-name $DATA_KEYVAULT_NAME | \\\n",
    "python -c 'import sys, json; \\ sys.stdout.write(json.load(sys.stdin)[0][\\\" value\\\"])')\n",
    "\n",
    "export WORKSPACE_STORAGE_KEY=$STORAGE_KEY\n",
    "\n",
    "## COPY DATA TO AZURE STORAGE IF NEEDED\n",
    "\n",
    "cd ~\n",
    "jupyter notebook\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment variables\n",
    "\n",
    "To run this sample notebook you need to have the following environment variables set to access the data:\n",
    "\n",
    "| Environment variable | Value expected | Example |\n",
    "| :--- | :--- | :--- |\n",
    "| DATA_STORAGE_ACCOUNT | The name of the storage account where dogbreeds is stored | msrsamplewedatast |\n",
    "| DATA_STORAGE_CONTAINER | The name of the storage account container where dogbreeds is stored | dogbreeds |\n",
    "| DATA_STORAGE_KEY | The access key to the dogbreeds account | [omitted] |\n",
    "\n",
    "To run this sample notebook you need to have the following environment variables set to access the workspace and the storage account:\n",
    "\n",
    "| Environment variable | Value expected | Example |\n",
    "| :--- | :--- | :--- |\n",
    "| SUBSCRIPTION_ID | The identifier for the subscription used to host your workspace | 0eae47bf-6f31-4db5-9754-1e2ddaac4d5a |\n",
    "| RESOURCEGROUP_NAME | The name of the resource group hosting your workspace | msr-demo3-westus2-res-rg  |\n",
    "| WORKSPACE_NAME | The name of your workspace | msrdemo3wu2ws |\n",
    "| WORSPACE_STORAGE_ACCOUNT | The name of the storage account associated with your workspace | msrdemo3wu2st |\n",
    "| WORSPACE_STORAGE_KEY | The access key of the storage account associated with your workspace | [omitted]  |\n",
    "\n",
    "NOTE: The DATA_STORAGE_ACCOUNT and WORKSPACE_STORAGE_ACCOUNT may be the same account. For Dogbreeds, the data is in a location shared by many users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need you can retrieve the workspace, subscription id and resource group name from the Azure portal.\n",
    "\n",
    "1. Log into Azure portal\n",
    "2. Click **All resources** \n",
    "3. Search on your department and team name\n",
    "\n",
    "![portal resources](assets/findworkspace.png)\n",
    "\n",
    "4. Click the workspace resource.\n",
    "\n",
    "![retrieve resource group and subscription](assets/retrievergandsub.png)\n",
    "\n",
    "5. Find the subscription ID, the resource group name, the storage account name, and the key vault name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.21\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dog breed classification using Pytorch Estimators on Azure Machine Learning service\n",
    "\n",
    "Have you ever seen a dog and not been able to tell the breed? Some dogs look so similar, that it can be nearly impossible to tell. For instance these are a few breeds that are difficult to tell apart:\n",
    "\n",
    "#### Alaskan Malamutes vs Siberian Huskies\n",
    "![Image of Alaskan Malamute vs Siberian Husky](http://cdn.akc.org/content/article-body-image/malamutehusky.jpg)\n",
    "\n",
    "#### Whippet vs Italian Greyhound \n",
    "![Image of Whippet vs Italian Greyhound](http://cdn.akc.org/content/article-body-image/whippetitalian.jpg)\n",
    "\n",
    "There are sites like http://what-dog.net, which use Microsoft Cognitive Services to be able to make this easier. \n",
    "\n",
    "In this tutorial, you will learn how to train a Pytorch image classification model using transfer learning with the Azure Machine Learning service. The Azure Machine Learning python SDK's [PyTorch estimator](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-pytorch) enables you to easily submit PyTorch training jobs for both single-node and distributed runs on Azure compute. The model is trained to classify dog breeds using the [Stanford Dog dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) and it is based on a pretrained ResNet18 model. This ResNet18 model has been built using images and annotation from ImageNet. The Stanford Dog dataset contains 120 classes (i.e. dog breeds), to save time however, for most of the tutorial, we will only use a subset of this dataset which includes only 10 dog breeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "At the start, the user is running her workload on their local machine and finds it is slow -- even thought they are only training on 8% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir outputs\n",
    "# !python pytorch_train.py --data_dir breeds-10 --num_epochs 10 --output_dir outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Azure Machine Learning service?\n",
    "Azure Machine Learning service is a cloud service that you can use to develop and deploy machine learning models. Using Azure Machine Learning service, you can track your models as you build, train, deploy, and manage them, all at the broad scale that the cloud provides.\n",
    "![](aml-overview.png)\n",
    "\n",
    "\n",
    "## How can we use it for training image classification models?\n",
    "Training machine learning models, particularly deep neural networks, is often a time- and compute-intensive task. Once you've finished writing your training script and running on a small subset of data on your local machine, you will likely want to scale up your workload.\n",
    "\n",
    "To facilitate training, the Azure Machine Learning Python SDK provides a high-level abstraction, the estimator class, which allows users to easily train their models in the Azure ecosystem. You can create and use an Estimator object to submit any training code you want to run on remote compute, whether it's a single-node run or distributed training across a GPU cluster. For PyTorch and TensorFlow jobs, Azure Machine Learning also provides respective custom PyTorch and TensorFlow estimators to simplify using these frameworks.\n",
    "\n",
    "### Steps to train with a Pytorch Estimator:\n",
    "In this tutorial, we will:\n",
    "- Connect to an Azure Machine Learning service Workspace \n",
    "- Create a remote compute target\n",
    "- Upload your training data (Optional)\n",
    "- Create your training script\n",
    "- Create an Estimator object\n",
    "- Submit your training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create workspace\n",
    "\n",
    "In the next step, you will create your own Workspace to use in this tutorial.\n",
    "\n",
    "**You will be asked to login during this step. Please use your Microsoft AAD credentials.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Make sure you have access to an Azure subscription. Your group's admin should have added you to your team's subscription. \n",
    "\n",
    "Details on how to set up the storage account are in the admin folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from azureml.core import Workspace\n",
    "#\n",
    "# ws = Workspace.from_config(path='/Users/danielsc/git/dogbreeds/aml_config/config.json')\n",
    "# \n",
    "# print('https://ms.portal.azure.com/#@microsoft.onmicrosoft.com/resource' + ws.get_details()['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: msrdemo8wetestws\n",
      "subscription_id: 710e04b9-9155-4f01-aa8e-52848f055ad2\n",
      "Resource group: msr-demo8-westeurope-test-rg\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "import os\n",
    "\n",
    "subscription_id = os.environ['SUBSCRIPTION_ID'].strip()\n",
    "resource_group_name  = os.environ['RESOURCEGROUP_NAME'].strip()\n",
    "workspace_name  = os.environ['WORKSPACE_NAME'].strip()\n",
    "\n",
    "print('Workspace name: ' + workspace_name, \n",
    "      'subscription_id: ' + subscription_id, \n",
    "      'Resource group: ' + resource_group_name , sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote the config file config.json to: C:\\Users\\v-brkyle\\msraidli\\dogbreeds\\team\\aml_config\\config.json\n",
      "Library configuration succeeded\n",
      "Workspace name: msrdemo8wetestws\n",
      "Azure region: westeurope\n",
      "Subscription id: 710e04b9-9155-4f01-aa8e-52848f055ad2\n",
      "Resource group: msr-demo8-westeurope-test-rg\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group_name, workspace_name = workspace_name)\n",
    "    ws.write_config()\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')\n",
    "\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This will take a few minutes, so let's talk about what a Workspace is while it is being created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k80-1gpus-low\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "target_list = ComputeTarget.list(ws)\n",
    "for target in target_list:\n",
    "    print(target.serialize()[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a remote compute target\n",
    "For this tutorial, we will create an AML Compute cluster with a NC6s_v2, P100 GPU machines, created to use as the [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) to execute your training script on. \n",
    "\n",
    "**Creation of the cluster takes approximately 5 minutes, but we will not wait for it to complete** \n",
    "\n",
    "If the cluster is already in your workspace this code will skip the cluster creation process. Note that the code is not waiting for completion of the cluster creation. If needed you can call `compute_target.wait_for_completion(show_output=True)`, which will block you notebook until the compute target is provisioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"k80-1gpus-low\"\n",
    "\n",
    "try:\n",
    "    compute_target = ws.compute_targets[cluster_name]\n",
    "    print('Found existing compute target.')\n",
    "except KeyError:\n",
    "     print('Cannot find existing compute target.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach the blobstore with the training data to the workspace\n",
    "While the cluster is still creating, let's attach some data to our workspace.\n",
    "\n",
    "The dataset we will use consists of ~150 images per class. Some breeds have more, while others have less. Each class has about 100 training images each for dog breeds, with ~50 validation images for each class. We will look at 10 classes in this tutorial.\n",
    "\n",
    "To make the data accessible for remote training, you will need to keep the data in the cloud. AML provides a convenient way to do so via a [Datastore](https://docs.microsoft.com/azure/machine-learning/service/how-to-access-data). The datastore provides a mechanism for you to upload/download data, and interact with it from your remote compute targets. It is an abstraction over Azure Storage. The datastore can reference either an Azure Blob container or Azure file share as the underlying storage. \n",
    "\n",
    "You can view the subset of the data used [here](https://github.com/heatherbshapiro/pycon-canada/tree/master/breeds-10). Or download it from [here](https://github.com/heatherbshapiro/pycon-canada/master/breeds-10.zip) as a zip file. \n",
    "\n",
    "We already copied the data to an Azure blob storage container. To attach this blob container as a data store to your workspace, you use the `Datastore.register_azure_blob_container` function. You can copy the statement with the secrets filled in from [here](https://microsoft.sharepoint.com/teams/azuremlnursery/_layouts/OneNote.aspx?id=%2Fteams%2Fazuremlnursery%2FSiteAssets%2FAzure%20ML%20Nursery%20Notebook&wd=target%28Workshop.one%7C265D85D5-44C8-9D40-B556-A31FA098E708%2FDogbreeds%7C62F09F92-105C-7849-AF84-905BEE9F9588%2F%29) (requires Microsoft Employee login).\n",
    "\n",
    "**If you already have the breeds datstore attached you can skip the next cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azureml.data.azure_storage_datastore.AzureBlobDatastore at 0x15993beb208>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Datastore\n",
    "import os\n",
    "\n",
    "Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                             datastore_name='thebreeds', \n",
    "                                             container_name=os.environ['DATA_STORAGE_CONTAINER'].strip(),\n",
    "                                             account_name=os.environ['DATA_STORAGE_ACCOUNT'].strip(), \n",
    "                                             account_key=os.environ['DATA_STORAGE_KEY'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get a reference to the path on the datastore with the training data. We can do so using the `path` method. In the next section, we can then pass this reference to our training script's `--data_dir` argument. We will start with the 10 classes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$AZUREML_DATAREFERENCE_65c0071bf2a44212be25192a2e3fa506\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "ds = Datastore(ws, 'thebreeds')\n",
    "\n",
    "path_on_datastore = 'breeds-10'\n",
    "ds_data = ds.path(path_on_datastore)\n",
    "print(ds_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up/Download Data\n",
    "\n",
    "If you are interested in downloading the data locally, you can run `ds.download(\".\", 'breeds-10')`. This might take several minutes.\n",
    "\n",
    "You can also upload your data. See [Upload to the datastore object](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.azure_storage_datastore.abstractazurestoragedatastore?view=azure-ml-py#upload-src-dir--target-path-none--overwrite-false--show-progress-true-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ds.upload('breeds-10', 'breeds-10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training script\n",
    "Now you will need to create your training script. In this tutorial, the training script is already provided for you at `pytorch_train.py`. In practice, you should be able to take any custom training script as is and run it with AML without having to modify your code.\n",
    "\n",
    "However, if you would like to use AML's [tracking and metrics](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#metrics) capabilities, you will have to add a small amount of AML code inside your training script. \n",
    "\n",
    "In `pytorch_train.py`, we will log some metrics to our AML run. To do so, we will access the AML run object within the script:\n",
    "```Python\n",
    "from azureml.core.run import Run\n",
    "run = Run.get_context()\n",
    "```\n",
    "Further within `pytorch_train.py`, we log the learning rate and momentum parameters, the best validation accuracy the model achieves, and the number of classes in the model:\n",
    "```Python\n",
    "run.log('lr', np.float(learning_rate))\n",
    "run.log('momentum', np.float(momentum))\n",
    "run.log('num_classes', num_classes)\n",
    "\n",
    "run.log('best_val_acc', np.float(best_acc))\n",
    "```\n",
    "\n",
    "If you downloaded the data, you can start to train the model locally (note that it will take long if you don't have a GPU -- 21 min. on a Core i7 CPU).\n",
    "\n",
    "**This step requires PyTorch to be installed locally -- find instructions [here](https://pytorch.org/#pip-install-pytorch)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir outputs\n",
    "# !python pytorch_train.py --data_dir breeds-10 --num_epochs 10 --output_dir outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the remote compute\n",
    "Now that you have your data and training script prepared, you are ready to train on your remote compute cluster. You can take advantage of Azure compute to leverage GPUs to cut down your training time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this transfer learning PyTorch tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment(Name: pytorch-dogs-1,\n",
      "Workspace: msrdemo8wetestws)\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'pytorch-dogs-1' \n",
    "experiment = Experiment(ws, name=experiment_name)\n",
    "\n",
    "print(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PyTorch estimator\n",
    "The AML SDK's PyTorch estimator enables you to easily submit PyTorch training jobs for both single-node and distributed runs. For more information on the PyTorch estimator, see [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-train-pytorch). \n",
    "\n",
    "The following code will define a single-node PyTorch job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "script_params = {\n",
    "    '--data_dir': ds_data.as_mount(),\n",
    "    '--num_epochs': 10,\n",
    "    '--output_dir': './outputs',\n",
    "    '--log_dir': './logs',\n",
    "    '--mode': 'fine_tune'\n",
    "}\n",
    "\n",
    "estimator10 = PyTorch(source_directory='.', \n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target, \n",
    "                    entry_script='pytorch_train.py',\n",
    "                    pip_packages=['tensorboardX'],\n",
    "                    use_gpu=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcr.microsoft.com/azureml/base-gpu:0.2.3\n"
     ]
    }
   ],
   "source": [
    "print(estimator10.run_config.environment.docker.base_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dependencies:\n",
      "- python=3.6.2\n",
      "- pip:\n",
      "  - azureml-defaults\n",
      "  - torch==1.0\n",
      "  - torchvision==0.2.1\n",
      "  - horovod==0.15.2\n",
      "  - tensorboardX\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(estimator10.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `script_params` parameter is a dictionary containing the command-line arguments to your training script `entry_script`. Please note the following:\n",
    "- Pass the training data reference `ds_data` to our script's `--data_dir` argument. This will 1) mount our datastore on the remote compute and 2) provide the path to the training data `breeds` on our datastore.\n",
    "- Specify the output directory as `./outputs`. The `outputs` directory is specially treated by AML in that all the content in this directory gets uploaded to your workspace as part of your run history. The files written to this directory are therefore accessible even once your remote run is over. In this tutorial, we will save our trained model to this output directory.\n",
    "\n",
    "To leverage the Azure VM's GPU for training, set `use_gpu=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-dogs-1_1557520149_5466c247\n"
     ]
    }
   ],
   "source": [
    "run = experiment.submit(estimator10)\n",
    "run_id = run.id\n",
    "\n",
    "print(run_id)\n",
    "\n",
    "## To cancel\n",
    "# run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e86c7dd24e4385bcba0b573965b993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens during a run?\n",
    "If you are running this for the first time, the compute target will need to pull the docker image, which will take about 2 minutes. This gives us the time to go over how a **Run** is executed in Azure Machine Learning. \n",
    "\n",
    "Note: had we not created the workspace with an existing ACR, we would have also had to wait for the image creation to be performed -- that takes and extra 10-20 minutes for big GPU images like this one. This is a one-time cost for a given python configuration, and subsequent runs will then be faster. We are working on ways to make this image creation faster.\n",
    "\n",
    "![](../aml-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tensorboard\n",
    "Tensorboard is a popular Deep Learning Training visualization tool. It is part of TensorFlow framework, but can be used from PyTorch as well by using TensorboadX python package. TensorboardX allows PyTorch to write metrics and log information in Tensorboard format.\n",
    "\n",
    "In `pytorch_train.py`, we will log metrics to \"magical\" `logs` directory. The content of this special directory is automatically streamed by Azue ML service. The logging into Tensorboard event format is performed by SummaryWriter object:\n",
    "```Python\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(f'./logs/{run.id}')\n",
    "```\n",
    "Within the script we write more detailed mini-batch loss and accuracy, as well as epoch level accuracy to Tensorboard:\n",
    "```Python\n",
    "writer.add_scalar(f'{phase}/Loss', loss.item(), niter)\n",
    "writer.add_scalar(f'{phase}/Epoch_accuracy', epoch_acc, (epoch+1) * len(dataloaders[phase]))\n",
    "```\n",
    "Finally, to ensure that TensorboardX python package is installed in Python environment during the training run, we add it using pip_packages parameter of the Estimator object:\n",
    "```Python\n",
    "estimator = PyTorch(...\n",
    "                    pip_packages=['tensorboardX']\n",
    "                    ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing and launching Tensorboard\n",
    "Azure ML SDK provides built-in integration with Tensorboard in package `azureml-contrib-tensorboard`, installed as part of the contrib extras package in prerequisites. In addition, you will need to pip install tensorboard, which we also did as part of the prerequisites.\n",
    "\n",
    "While the run is in progress (or after it has completed), we just need to start Tensorboard with the run as its target, and it will begin streaming logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.tensorboard import Tensorboard\n",
    "\n",
    "# The Tensorboard constructor takes an array of runs, so be sure and pass it in as a single-element array here\n",
    "tb = Tensorboard([run10])\n",
    "\n",
    "# If successful, start() returns a string with the URI of the instance.\n",
    "tb.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Tensorboard\n",
    "\n",
    "When you're done, make sure to call the `stop()` method of the Tensorboard object, or it will stay running even after your job completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track job in the portal\n",
    "\n",
    "Once the job has completed, the results are copied into your storage account.\n",
    "Inspect the portal for the results using the link in the widget in the previous cell.\n",
    "\n",
    "![results](assets/results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get results\n",
    "\n",
    "You can get the logs and models from:\n",
    "\n",
    "- The portal\n",
    "- By inspecting results in Azure storage\n",
    "- By downloading the logs\n",
    "\n",
    "### Your results in portal \n",
    "\n",
    "For the models\n",
    "\n",
    "![portal output](assets/outputs.png) \n",
    "\n",
    "For the logs\n",
    "\n",
    "![logs](assets/logs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../results/azureml-logs/60_control_log.txt',\n",
       " '../../results/azureml-logs/80_driver_log.txt',\n",
       " '../../results/azureml-logs/azureml.log',\n",
       " '../../results/azureml-logs/55_batchai_execution.txt']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Download all logs to a local directory\n",
    "\n",
    "run.get_all_logs(destination='../../results/')\n",
    "\n",
    "# do not put them into a directory in the same location as your script. \n",
    "# If you do, the next time you run your run, it will upload it to Azure the next time you run your Python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-dogs-1_1557520149_5466c247\n"
     ]
    }
   ],
   "source": [
    "print(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your results in Azure storage\n",
    "\n",
    "The results are in workspace's storage account (not the $AZURE_STORAGE_ACCOUNT where you got the data from).\n",
    "The `<run_id>` is your run_id set in a previous cell.\n",
    "\n",
    "`http://$WORKSPACE_STORAGE_ACCOUNT.blob.core.windows.net/azureml/ExperimentRun/dcid.<run_id>/outputs`\n",
    "\n",
    "![data in storage](assets/datainstorage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Continue Dogbreeds sample code to:\n",
    "\n",
    "- Distributed training\n",
    "- Hyperparameter tuning\n",
    "- Azure Machine Learning Pipelines\n",
    "- Inferencing\n",
    "- Deploy model as web service\n",
    "\n",
    "Copy your data to your storage account using AzCopy into your workspace storage.\n",
    "\n",
    "```bash\n",
    "STORAGE_CONTAINER_NAME=\"<folder name for your data>\"\n",
    "\n",
    "azcopy \\ \n",
    "  --source /mnt/myfiles/ \\ \n",
    "  --destination https://$WORKSPACE_STORAGE_ACCOUNT.blob.core.windows.net/$STORAGE_CONTAINER_NAME \\     \n",
    "  --dest-key $WORKSPACE_STORAGE_KEY \\ \n",
    "  --recursive\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
