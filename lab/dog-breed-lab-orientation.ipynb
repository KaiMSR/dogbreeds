{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Azure Machine Learning services\n",
    "\n",
    "_Special thanks to the Azure CAT team and Daniel for the Dogbreeds sample._\n",
    "\n",
    "The purpose of this sample is to demonstrate:\n",
    "\n",
    "- What you need to start your run once the administrator has set up a workspaces for you\n",
    "- How to start your Azure Machine Learning services run \n",
    "\n",
    "This set up notebook shows only the first feature of \n",
    "See [Daniel's dogbreed notebook](../) for:\n",
    "\n",
    "- Distributed training\n",
    "- Hyperparameter tuning\n",
    "- Azure Machine Learning Pipelines\n",
    "- Inferencing\n",
    "- Deploy model as web service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System overview\n",
    "\n",
    "You run your application from the local development system and submit it to an Azure workspace that consists of:\n",
    "\n",
    "- Workspace\n",
    "- AML Compute\n",
    "- Storage\n",
    "- Key vault\n",
    "- Container registry\n",
    "- Applications Insight\n",
    "\n",
    "The lab shares a common storage where the Dogbreeds demo data is stored. You will access the key to the lab storage using Key Vault.\n",
    "\n",
    "![overview](assets/overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admin provides you the workspace info:\n",
    "\n",
    "- SUBSCRIPTION_ID\n",
    "- RESOURCEGROUP_NAME\n",
    "- WORKSPACE_NAME\n",
    "- CLUSTER_NAME\n",
    "\n",
    "Admin gives you the shared data store information for the Dogbreeds data:\n",
    "\n",
    "- DATA_STORAGE_RESOURCE_GROUP\n",
    "- DATA_STORAGE_ACCOUNT\n",
    "- DATA_STORAGE_CONTAINER\n",
    "\n",
    "(Before starting this notebook, you will used information provided by the admin to retrieve the storage key, such as:\n",
    "\n",
    "```\n",
    "export DATA_STORAGE_KEY=$(az storage account keys list -g $DATA_RESOURCE_GROUP \\\n",
    "\t  -n $DATA_STORAGE_ACCOUNT --query [0].value | tr -d '\"')\n",
    "```\n",
    "\n",
    "Set each of these variables as shown in the next cell are you are ready to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "subscription_id = os.environ['SUBSCRIPTION_ID']\n",
    "resource_group_name  = os.environ['RESOURCE_GROUP']\n",
    "workspace_name  = os.environ['WORKSPACE']\n",
    "cluster_name = os.environ['CLUSTER_NAME']\n",
    "\n",
    "print('Workspace name: ' + workspace_name, \n",
    "      'subscription_id: ' + subscription_id, \n",
    "      'Resource group: ' + resource_group_name , sep = '\\n')\n",
    "\n",
    "data_storage_resource_group = os.environ['DATA_RESOURCE_GROUP']\n",
    "data_storage_account = os.environ['DATA_STORAGE_ACCOUNT']\n",
    "data_storage_container= os.environ['DATA_STORAGE_CONTAINER']\n",
    "key=os.environ['DATA_STORAGE_KEY']\n",
    "\n",
    "print('Data storage resource group: ' + data_storage_resource_group, \n",
    "      'Data storage account: ' + data_storage_account, \n",
    "      'Data storage container: ' + data_storage_container , sep = '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data storage provides a common storage location for data shared in the lab. You can also replace the values for your own data storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dog breed classification using Pytorch Estimators on Azure Machine Learning service\n",
    "\n",
    "Have you ever seen a dog and not been able to tell the breed? Some dogs look so similar, that it can be nearly impossible to tell. For instance these are a few breeds that are difficult to tell apart:\n",
    "\n",
    "#### Alaskan Malamutes vs Siberian Huskies\n",
    "![Image of Alaskan Malamute vs Siberian Husky](http://cdn.akc.org/content/article-body-image/malamutehusky.jpg)\n",
    "\n",
    "#### Whippet vs Italian Greyhound \n",
    "![Image of Whippet vs Italian Greyhound](http://cdn.akc.org/content/article-body-image/whippetitalian.jpg)\n",
    "\n",
    "There are sites like http://what-dog.net, which use Microsoft Cognitive Services to be able to make this easier. \n",
    "\n",
    "In this tutorial, you will learn how to train a Pytorch image classification model using transfer learning with the Azure Machine Learning service. The Azure Machine Learning python SDK's [PyTorch estimator](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-pytorch) enables you to easily submit PyTorch training jobs for both single-node and distributed runs on Azure compute. The model is trained to classify dog breeds using the [Stanford Dog dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) and it is based on a pretrained ResNet18 model. This ResNet18 model has been built using images and annotation from ImageNet. The Stanford Dog dataset contains 120 classes (i.e. dog breeds), to save time however, for most of the tutorial, we will only use a subset of this dataset which includes only 10 dog breeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "At the start, the user is running her workload on their local machine and finds it is slow -- even thought they are only training on 8% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir outputs\n",
    "# !python pytorch_train.py --data_dir breeds-10 --num_epochs 10 --output_dir outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Azure Machine Learning service?\n",
    "Azure Machine Learning service is a cloud service that you can use to develop and deploy machine learning models. Using Azure Machine Learning service, you can track your models as you build, train, deploy, and manage them, all at the broad scale that the cloud provides.\n",
    "![](aml-overview.png)\n",
    "\n",
    "\n",
    "## How can we use it for training image classification models?\n",
    "Training machine learning models, particularly deep neural networks, is often a time- and compute-intensive task. Once you've finished writing your training script and running on a small subset of data on your local machine, you will likely want to scale up your workload.\n",
    "\n",
    "To facilitate training, the Azure Machine Learning Python SDK provides a high-level abstraction, the estimator class, which allows users to easily train their models in the Azure ecosystem. You can create and use an Estimator object to submit any training code you want to run on remote compute, whether it's a single-node run or distributed training across a GPU cluster. For PyTorch and TensorFlow jobs, Azure Machine Learning also provides respective custom PyTorch and TensorFlow estimators to simplify using these frameworks.\n",
    "\n",
    "### Steps to train with a Pytorch Estimator:\n",
    "In this tutorial, we will:\n",
    "- Connect to an Azure Machine Learning service Workspace \n",
    "- Create a remote compute target\n",
    "- Upload your training data (Optional)\n",
    "- Create your training script\n",
    "- Create an Estimator object\n",
    "- Submit your training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create workspace\n",
    "\n",
    "In the next step, you will create your own Workspace to use in this tutorial.\n",
    "\n",
    "**You will be asked to login during this step. Please use your Microsoft AAD credentials.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Make sure you have access to an Azure subscription. Your group's admin should have added you to your team's subscription. \n",
    "\n",
    "Details on how to set up the storage account are in the admin folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from azureml.core import Workspace\n",
    "#\n",
    "# ws = Workspace.from_config(path='/Users/danielsc/git/dogbreeds/aml_config/config.json')\n",
    "# \n",
    "# print('https://ms.portal.azure.com/#@microsoft.onmicrosoft.com/resource' + ws.get_details()['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "import os\n",
    "\n",
    "print('Workspace name: ' + workspace_name, \n",
    "      'subscription_id: ' + subscription_id, \n",
    "      'Resource group: ' + resource_group_name , sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group_name, workspace_name = workspace_name)\n",
    "    ws.write_config()\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')\n",
    "\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This will take a few minutes, so let's talk about what a Workspace is while it is being created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "target_list = ComputeTarget.list(ws)\n",
    "for target in target_list:\n",
    "    print(target.serialize()[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a remote compute target\n",
    "For this tutorial, we will create an AML Compute cluster with a NC6s_v2, P100 GPU machines, created to use as the [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) to execute your training script on. \n",
    "\n",
    "**Creation of the cluster takes approximately 5 minutes, but we will not wait for it to complete** \n",
    "\n",
    "If the cluster is already in your workspace this code will skip the cluster creation process. Note that the code is not waiting for completion of the cluster creation. If needed you can call `compute_target.wait_for_completion(show_output=True)`, which will block you notebook until the compute target is provisioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "# choose a name for your cluster\n",
    "print('cluster_name: ' + cluster_name) \n",
    "\n",
    "try:\n",
    "    compute_target = ws.compute_targets[cluster_name]\n",
    "    print('Found existing compute target.')\n",
    "except KeyError:\n",
    "     print('Cannot find existing compute target.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach the blobstore with the training data to the workspace\n",
    "While the cluster is still creating, let's attach some data to our workspace.\n",
    "\n",
    "The dataset we will use consists of ~150 images per class. Some breeds have more, while others have less. Each class has about 100 training images each for dog breeds, with ~50 validation images for each class. We will look at 10 classes in this tutorial.\n",
    "\n",
    "To make the data accessible for remote training, you will need to keep the data in the cloud. AML provides a convenient way to do so via a [Datastore](https://docs.microsoft.com/azure/machine-learning/service/how-to-access-data). The datastore provides a way for you to upload/download data, and interact with it from your remote compute targets. It is an abstraction over Azure Storage. The datastore can reference either an Azure Blob container or Azure file share as the underlying storage. \n",
    "\n",
    "You can view the subset of the data used [here](https://github.com/heatherbshapiro/pycon-canada/tree/master/breeds-10). Or download it from [here](https://github.com/heatherbshapiro/pycon-canada/master/breeds-10.zip) as a zip file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To attach this blob container as a data store to your workspace, use  `Datastore.register_azure_blob_container`. \n",
    "\n",
    "**If you already have the breeds datstore attached you can skip the next cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "import os\n",
    "\n",
    "key = key.strip('\\\"\"')\n",
    "\n",
    "print('Data storage resource group: ' + data_storage_resource_group, \n",
    "      'Data storage account: ' + data_storage_account, \n",
    "      'Data storage container: ' + data_storage_container , sep = '\\n')\n",
    "\n",
    "Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                        datastore_name='thebreeds', \n",
    "                                        container_name=\"dogbreeds\",\n",
    "                                        account_name=data_storage_account, \n",
    "                                        account_key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get a reference to the path on the datastore with the training data. We can do so using the `path` method. In the next section, we can then pass this reference to our training script's `--data_dir` argument. We will start with the 10 classes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "ds = Datastore(ws, 'thebreeds')\n",
    "\n",
    "path_on_datastore = 'breeds-10'\n",
    "ds_data = ds.path(path_on_datastore)\n",
    "print(ds_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up/Download Data\n",
    "\n",
    "If you are interested in downloading the data locally, you can run `ds.download(\".\", 'breeds-10')`. This might take several minutes.\n",
    "\n",
    "You can also upload your data. See [Upload to the datastore object](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.azure_storage_datastore.abstractazurestoragedatastore?view=azure-ml-py#upload-src-dir--target-path-none--overwrite-false--show-progress-true-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ds.upload('breeds-10', 'breeds-10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training script\n",
    "Now you will need to create your training script. In this tutorial, the training script is already provided for you at `pytorch_train.py`. In practice, you should be able to take any custom training script as is and run it with AML without having to modify your code.\n",
    "\n",
    "You will need to access to your data and define a location for your output.\n",
    "\n",
    "#### Training and metrics\n",
    "\n",
    "However, if you would like to use AML's [tracking and metrics](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#metrics) capabilities, you will have to add a small amount of AML code inside your training script. \n",
    "\n",
    "In `pytorch_train.py`, we will log some metrics to our AML run. To do so, we will access the AML run object within the script:\n",
    "```Python\n",
    "from azureml.core.run import Run\n",
    "run = Run.get_context()\n",
    "```\n",
    "Further within `pytorch_train.py`, we log the learning rate and momentum parameters, the best validation accuracy the model achieves, and the number of classes in the model:\n",
    "```Python\n",
    "run.log('lr', np.float(learning_rate))\n",
    "run.log('momentum', np.float(momentum))\n",
    "run.log('num_classes', num_classes)\n",
    "\n",
    "run.log('best_val_acc', np.float(best_acc))\n",
    "```\n",
    "\n",
    "If you downloaded the data, you can start to train the model locally (note that it will take long if you don't have a GPU -- 21 min. on a Core i7 CPU).\n",
    "\n",
    "**This step requires PyTorch to be installed locally -- find instructions [here](https://pytorch.org/#pip-install-pytorch)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir outputs\n",
    "# !python pytorch_train.py --data_dir breeds-10 --num_epochs 10 --output_dir outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the remote compute\n",
    "Now that you have your data and training script prepared, you are ready to train on your remote compute cluster. You can take advantage of Azure compute to leverage GPUs to cut down your training time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this transfer learning PyTorch tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'pytorch-dogs-10' \n",
    "experiment = Experiment(ws, name=experiment_name)\n",
    "\n",
    "print(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PyTorch estimator\n",
    "The AML SDK's PyTorch estimator enables you to easily submit PyTorch training jobs for both single-node and distributed runs. For more information on the PyTorch estimator, see [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-train-pytorch). \n",
    "\n",
    "The following code will define a single-node PyTorch job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "script_params = {\n",
    "    '--data_dir': ds_data.as_mount(),\n",
    "    '--num_epochs': 10,\n",
    "    '--output_dir': './outputs',\n",
    "    '--log_dir': './logs',\n",
    "    '--mode': 'fine_tune'\n",
    "}\n",
    "\n",
    "estimator10 = PyTorch(source_directory='.', \n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target, \n",
    "                    entry_script='pytorch_train.py',\n",
    "                    pip_packages=['tensorboardX'],\n",
    "                    use_gpu=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimator10.run_config.environment.docker.base_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimator10.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `script_params` parameter is a dictionary containing the command-line arguments to your training script `entry_script`. Please note the following:\n",
    "- Pass the training data reference `ds_data` to our script's `--data_dir` argument. This will 1) mount our datastore on the remote compute and 2) provide the path to the training data `breeds` on our datastore.\n",
    "- Specify the output directory as `./outputs`. The `outputs` directory is specially treated by AML in that all the content in this directory gets uploaded to your workspace as part of your run history. The files written to this directory are therefore accessible even once your remote run is over. In this tutorial, we will save our trained model to this output directory.\n",
    "\n",
    "To leverage the Azure VM's GPU for training, set `use_gpu=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag the run\n",
    "\n",
    "Before submitting the experiment, tag your run. Add information you will need to find your run at a later time. \n",
    "\n",
    "The tags are a Python dictionary of your own choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogtags = {'Run':'Dogbreeds', 'Source':'Notebook', 'Researcher':'Bruce', \"Datasize\" : path_on_datastore}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(estimator10, tags=dogtags)\n",
    "run_id = run.id\n",
    "\n",
    "print(run_id)\n",
    "\n",
    "## To cancel\n",
    "# run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens during a run?\n",
    "If you are running this for the first time, the compute target will need to pull the docker image, which will take about 2 minutes. This gives us the time to go over how a **Run** is executed in Azure Machine Learning. \n",
    "\n",
    "Note: had we not created the workspace with an existing ACR, we would have also had to wait for the image creation to be performed -- that takes and extra 10-20 minutes for big GPU images like this one. This is a one-time cost for a given python configuration, and subsequent runs will then be faster. We are working on ways to make this image creation faster.\n",
    "\n",
    "![](../aml-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track job in the portal\n",
    "\n",
    "Once the job has completed, the results are copied into your storage account.\n",
    "Inspect the portal for the results using the link in the widget in the previous cell.\n",
    "\n",
    "![results](assets/results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get results\n",
    "\n",
    "You can get the logs and models from:\n",
    "\n",
    "- The portal\n",
    "- By inspecting results in Azure storage\n",
    "- By downloading the logs\n",
    "\n",
    "### Your results in portal \n",
    "\n",
    "For the models\n",
    "\n",
    "![portal output](assets/outputs.png) \n",
    "\n",
    "For the logs\n",
    "\n",
    "![logs](assets/logs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download all logs to a local directory\n",
    "\n",
    "run.get_all_logs(destination='../../results/')\n",
    "\n",
    "# do not put them into a directory in the same location as your script. \n",
    "# If you do, the next time you run your run, it will upload it to Azure the next time you run your Python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ws.get_details())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your results in Azure storage\n",
    "\n",
    "The results are in workspace's storage account (not the $DATA_STORAGE_ACCOUNT where you got the data from).\n",
    "The `<run_id>` is your run_id set in a previous cell.\n",
    "\n",
    "`http://$WORKSPACE_STORAGE_ACCOUNT.blob.core.windows.net/azureml/ExperimentRun/dcid.<run_id>/outputs`\n",
    "\n",
    "![data in storage](assets/datainstorage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Continue Dogbreeds sample code to:\n",
    "\n",
    "- Distributed training\n",
    "- Hyperparameter tuning\n",
    "- Azure Machine Learning Pipelines\n",
    "- Inferencing\n",
    "- Deploy model as web service\n",
    "\n",
    "Copy your data to your storage account using AzCopy into your workspace storage.\n",
    "\n",
    "```bash\n",
    "STORAGE_CONTAINER_NAME=\"<folder name for your data>\"\n",
    "\n",
    "azcopy \\ \n",
    "  --source /mnt/myfiles/ \\ \n",
    "  --destination https://$WORKSPACE_STORAGE_ACCOUNT.blob.core.windows.net/$STORAGE_CONTAINER_NAME \\     \n",
    "  --dest-key $WORKSPACE_STORAGE_KEY \\ \n",
    "  --recursive\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
